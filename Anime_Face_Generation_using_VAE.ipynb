{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "jupytext": {
      "encoding": "# -*- coding: utf-8 -*-"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "name": "Anime Face Generation using VAE.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TanishkaMarrott/DeepAnime-Generate-Animes-using-VAEs/blob/main/Anime_Face_Generation_using_VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prTKL3d2kGZE"
      },
      "source": [
        "#  Variational Autoencoders on Anime Faces\n",
        "\n",
        "Training a Variational Autoencoder (VAE) using the anime faces dataset by McKinsey"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Qxq9uZAk3Lh"
      },
      "source": [
        "## Importing the necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MooRFGEeI1zb"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "import urllib.request\n",
        "import random\n",
        "from IPython import display"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wL9rq-0uk7nS"
      },
      "source": [
        "## Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjhN6GgfmUfx"
      },
      "source": [
        "# setting a random seed\n",
        "np.random.seed(51)\n",
        "\n",
        "# parameters for building the model and training\n",
        "BATCH_SIZE=2000\n",
        "LATENT_DIM=512\n",
        "IMAGE_SIZE=64"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXTdjxmolDBo"
      },
      "source": [
        "Downloading the Anime Face Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxKW6Q88KHcL"
      },
      "source": [
        "# make the data directory\n",
        "try:\n",
        "  os.mkdir('/tmp/anime')\n",
        "except OSError:\n",
        "  pass\n",
        "\n",
        "# download the zipped dataset to the data directory\n",
        "data_url = \"https://storage.googleapis.com/laurencemoroney-blog.appspot.com/Resources/anime-faces.zip\"\n",
        "data_file_name = \"animefaces.zip\"\n",
        "download_dir = '/tmp/anime/'\n",
        "urllib.request.urlretrieve(data_url, data_file_name)\n",
        "\n",
        "# extract the zip file\n",
        "zip_ref = zipfile.ZipFile(data_file_name, 'r')\n",
        "zip_ref.extractall(download_dir)\n",
        "zip_ref.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kD6WCIlclWaA"
      },
      "source": [
        "## Data Preparation Utilities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbaVpD18ggOX"
      },
      "source": [
        "Preparing the data for Training and Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTlx97U_JDPB"
      },
      "source": [
        "# Data Preparation Utilities\n",
        "\n",
        "def get_dataset_slice_paths(image_dir):\n",
        "  '''returns a list of paths to the image files'''\n",
        "  image_file_list = os.listdir(image_dir)\n",
        "  image_paths = [os.path.join(image_dir, fname) for fname in image_file_list]\n",
        "\n",
        "  return image_paths\n",
        "\n",
        "\n",
        "def map_image(image_filename):\n",
        "  '''preprocesses the images'''\n",
        "  img_raw = tf.io.read_file(image_filename)\n",
        "  image = tf.image.decode_jpeg(img_raw)\n",
        "\n",
        "  image = tf.cast(image, dtype=tf.float32)\n",
        "  image = tf.image.resize(image, (IMAGE_SIZE, IMAGE_SIZE))\n",
        "  image = image / 255.0  \n",
        "  image = tf.reshape(image, shape=(IMAGE_SIZE, IMAGE_SIZE, 3,))\n",
        "\n",
        "  return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uFon6vdhMhi"
      },
      "source": [
        "Generating the train and validation sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGoCJ6DPJHL8"
      },
      "source": [
        "# get the list containing the image paths\n",
        "paths = get_dataset_slice_paths(\"/tmp/anime/images/\")\n",
        "\n",
        "# shuffle the paths\n",
        "random.shuffle(paths)\n",
        "\n",
        "# split the paths list into to training (80%) and validation sets(20%).\n",
        "paths_len = len(paths)\n",
        "train_paths_len = int(paths_len * 0.8)\n",
        "\n",
        "train_paths = paths[:train_paths_len]\n",
        "val_paths = paths[train_paths_len:]\n",
        "\n",
        "# load the training image paths into tensors, create batches and shuffle\n",
        "training_dataset = tf.data.Dataset.from_tensor_slices((train_paths))\n",
        "training_dataset = training_dataset.map(map_image)\n",
        "training_dataset = training_dataset.shuffle(1000).batch(BATCH_SIZE)\n",
        "\n",
        "# load the validation image paths into tensors and create batches\n",
        "validation_dataset = tf.data.Dataset.from_tensor_slices((val_paths))\n",
        "validation_dataset = validation_dataset.map(map_image)\n",
        "validation_dataset = validation_dataset.batch(BATCH_SIZE)\n",
        "\n",
        "\n",
        "print(f'number of batches in the training set: {len(training_dataset)}')\n",
        "print(f'number of batches in the validation set: {len(validation_dataset)}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72ZRga9vlonx"
      },
      "source": [
        "Display utilities to aid Data Visualisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jC1cpLViJLIu"
      },
      "source": [
        "def display_faces(dataset, size=9):\n",
        "  '''Takes a sample from a dataset batch and plots it in a grid.'''\n",
        "  dataset = dataset.unbatch().take(size)\n",
        "  n_cols = 3\n",
        "  n_rows = size//n_cols + 1\n",
        "  plt.figure(figsize=(5, 5))\n",
        "  i = 0\n",
        "  \n",
        "  for image in dataset:\n",
        "    i += 1\n",
        "    disp_img = np.reshape(image, (64,64,3))\n",
        "    plt.subplot(n_rows, n_cols, i)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.imshow(disp_img)\n",
        "\n",
        "\n",
        "def display_one_row(disp_images, offset, shape=(28, 28)):\n",
        "  '''Displays a row of images.'''\n",
        "  for idx, image in enumerate(disp_images):\n",
        "    plt.subplot(3, 10, offset + idx + 1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    image = np.reshape(image, shape)\n",
        "    plt.imshow(image)\n",
        "\n",
        "\n",
        "def display_results(disp_input_images, disp_predicted):\n",
        "  '''Displays input and predicted images.'''\n",
        "  plt.figure(figsize=(15, 5))\n",
        "  display_one_row(disp_input_images, 0, shape=(IMAGE_SIZE,IMAGE_SIZE,3))\n",
        "  display_one_row(disp_predicted, 20, shape=(IMAGE_SIZE,IMAGE_SIZE,3))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2brROh6qLJbs"
      },
      "source": [
        "Checking out some samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eZsrZtqJOzv"
      },
      "source": [
        "display_faces(validation_dataset, size=12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSBtdCVim9aC"
      },
      "source": [
        "## Build the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHNxIUUS9ng9"
      },
      "source": [
        "### Sampling Class\n",
        "\n",
        "Custom layer to provide the Gaussian Noise input along with the mean (mu) and standard deviation (sigma)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-3qk6ZBm0Fl"
      },
      "source": [
        "class Sampling(tf.keras.layers.Layer):\n",
        "  def call(self, inputs):\n",
        "    \"\"\"Generates a random sample and combines with the encoder output\n",
        "    \n",
        "    Args:\n",
        "      inputs -- output tensor from the encoder\n",
        "\n",
        "    Returns:\n",
        "      `inputs` tensors combined with a random sample\n",
        "    \"\"\"\n",
        "    mu, sigma = inputs \n",
        "    batch = tf.shape(mu)[0]\n",
        "    dim = tf.shape(mu)[1]\n",
        "    epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "    z = mu + tf.exp(0.5 * sigma) * epsilon\n",
        "    return  z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZjCSa7Y-Gvk"
      },
      "source": [
        "### Encoder Layers\n",
        "\n",
        "Using the Functional API to stack the encoder layers and output `mu`, `sigma` and the shape of the features before flattening. \n",
        "\n",
        "Using 3 convolutional layers.\n",
        "and `1024` units in the Dense layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VSVYjDim4Dk"
      },
      "source": [
        "def encoder_layers(inputs, latent_dim):\n",
        "  \"\"\"Defines the encoder's layers.\n",
        "  Args:\n",
        "    inputs -- batch from the dataset\n",
        "    latent_dim -- dimensionality of the latent space\n",
        "\n",
        "  Returns:\n",
        "    mu -- learned mean\n",
        "    sigma -- learned standard deviation\n",
        "    batch_3.shape -- shape of the features before flattening\n",
        "  \"\"\"\n",
        "\n",
        "  x = tf.keras.layers.Conv2D(filters=32, kernel_size=3, strides=2, padding=\"same\", activation='relu', name=\"encode_conv1\")(inputs)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  \n",
        "\n",
        "  x = tf.keras.layers.Conv2D(filters=64, kernel_size=3, strides=2, padding='same', activation='relu', name=\"encode_conv2\")(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "\n",
        "  x = tf.keras.layers.Conv2D(filters=128, kernel_size=3, strides=2, padding='same', activation='relu', name=\"encode_conv3\")(x) \n",
        "\n",
        "  # assign to a different variable to extract the shape\n",
        "  batch_3 = tf.keras.layers.BatchNormalization()(x)   \n",
        "\n",
        "  # flatten the features and feed into the Dense network\n",
        "  x = tf.keras.layers.Flatten(name=\"encode_flatten\")(batch_3)\n",
        "\n",
        "  # Dense units\n",
        "  x = tf.keras.layers.Dense(1024, activation='relu', name=\"encode_dense\")(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "  # adding output Dense networks for mu and sigma, units equal to the declared latent_dim.\n",
        "  mu = tf.keras.layers.Dense(latent_dim, name='latent_mu')(x)\n",
        "  sigma = tf.keras.layers.Dense(latent_dim, name ='latent_sigma')(x)\n",
        "  \n",
        "\n",
        "  return mu, sigma, batch_3.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOy7wPPY-g-N"
      },
      "source": [
        "### Encoder Model\n",
        "\n",
        "You will feed the output from the above function to the `Sampling layer` you defined earlier. That will have the latent representations that can be fed to the decoder network later. ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8Y-wLFym60N"
      },
      "source": [
        "def encoder_model(latent_dim, input_shape):\n",
        "  \"\"\"Defines the encoder model with the Sampling layer\n",
        "  Args:\n",
        "    latent_dim -- dimensionality of the latent space\n",
        "    input_shape -- shape of the dataset batch\n",
        "\n",
        "  Returns:\n",
        "    model -- the encoder model\n",
        "    conv_shape -- shape of the features before flattening\n",
        "  \"\"\"\n",
        "  \n",
        "  inputs = tf.keras.layers.Input(shape=input_shape)\n",
        "  mu, sigma, conv_shape =  encoder_layers(inputs, latent_dim=LATENT_DIM)\n",
        "  z = Sampling()((mu, sigma))\n",
        "  model = tf.keras.Model(inputs, outputs=[mu, sigma, z])\n",
        "\n",
        "\n",
        "  model.summary()\n",
        "  return model, conv_shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9ENB-6a-0R5"
      },
      "source": [
        "### Decoder Layers\n",
        "\n",
        "Defining the decoder layers. This will expand the latent representations back to the original image dimensions. \n",
        "\n",
        "After training VAE model, we'll use the decoder model to generate new data by feeding random inputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlTjAzgsm9Vn"
      },
      "source": [
        "def decoder_layers(inputs, conv_shape):\n",
        "  \"\"\"Defines the decoder layers.\n",
        "  Args:\n",
        "    inputs -- output of the encoder \n",
        "    conv_shape -- shape of the features before flattening\n",
        "\n",
        "  Returns:\n",
        "    tensor containing the decoded output\n",
        "  \"\"\"\n",
        "  \n",
        "  # feed to a Dense network with units computed from the conv_shape dimensions\n",
        "  units = conv_shape[1] * conv_shape[2] * conv_shape[3]\n",
        "  x = tf.keras.layers.Dense(units, activation = 'relu', name=\"decode_dense1\")(inputs)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  \n",
        "  # reshape output using the conv_shape dimensions\n",
        "  x = tf.keras.layers.Reshape((conv_shape[1], conv_shape[2], conv_shape[3]), name=\"decode_reshape\")(x)\n",
        "\n",
        "\n",
        "\n",
        "  # upsample the features back to the original dimensions \n",
        "  x = tf.keras.layers.Conv2DTranspose(filters=128, kernel_size=3, strides=2, padding='same', activation='relu', name=\"decode_conv2d_1\")(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  \n",
        "  x = tf.keras.layers.Conv2DTranspose(filters=64, kernel_size=3, strides=2, padding='same', activation='relu', name=\"decode_conv2d_12\")(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x)\n",
        "  \n",
        "  x = tf.keras.layers.Conv2DTranspose(filters=32, kernel_size=3, strides=2, padding='same', activation='relu', name=\"decode_conv2d_2\")(x)\n",
        "  x = tf.keras.layers.BatchNormalization()(x) \n",
        "\n",
        "  x = tf.keras.layers.Conv2DTranspose(filters=3, kernel_size=3, strides=1, padding='same', activation='sigmoid', name=\"decode_final\")(x)\n",
        "  \n",
        " \n",
        "  return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfLLz84r_MlN"
      },
      "source": [
        "### Decoder Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUgTyqNFm_jR"
      },
      "source": [
        "def decoder_model(latent_dim, conv_shape):\n",
        "  \"\"\"Defines the decoder model.\n",
        "  Args:\n",
        "    latent_dim -- dimensionality of the latent space\n",
        "    conv_shape -- shape of the features before flattening\n",
        "\n",
        "  Returns:\n",
        "    model -- the decoder model\n",
        "  \"\"\"\n",
        "  \n",
        "  inputs = tf.keras.layers.Input(shape=(latent_dim,))\n",
        "  outputs = decoder_layers(inputs, conv_shape)\n",
        "  model = tf.keras.Model(inputs, outputs)\n",
        "  \n",
        "  model.summary()\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ps0yuE1d_cQc"
      },
      "source": [
        "### Kullback–Leibler Divergence\n",
        "\n",
        "Define the function to compute the KL Divergence\n",
        "(https://arxiv.org/abs/2002.07514) loss. This will be used to improve the generative capability of the model. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tngFmDDwnDn-"
      },
      "source": [
        "def kl_reconstruction_loss(inputs, outputs, mu, sigma):\n",
        "  \"\"\" Computes the Kullback-Leibler Divergence (KLD)\n",
        "  Args:\n",
        "    inputs -- batch from the dataset\n",
        "    outputs -- output of the Sampling layer\n",
        "    mu -- mean\n",
        "    sigma -- standard deviation\n",
        "\n",
        "  Returns:\n",
        "    KLD loss\n",
        "  \"\"\"\n",
        "  kl_loss = 1 + sigma - tf.square(mu) - tf.math.exp(sigma)\n",
        "  return tf.reduce_mean(kl_loss) * -0.5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pi1I431I_og7"
      },
      "source": [
        "### Assembling the whole VAE Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuPHg28JnGCp"
      },
      "source": [
        "def vae_model(encoder, decoder, input_shape):\n",
        "  \"\"\"Defines the VAE model\n",
        "  Args:\n",
        "    encoder -- the encoder model\n",
        "    decoder -- the decoder model\n",
        "    input_shape -- shape of the dataset batch\n",
        "\n",
        "  Returns:\n",
        "    the complete VAE model\n",
        "  \"\"\"\n",
        "  inputs = tf.keras.layers.Input(shape=input_shape)\n",
        "\n",
        "  # get mu, sigma, and z from the encoder output\n",
        "  mu, sigma, z = encoder(inputs)\n",
        "  \n",
        "  # get reconstructed output from the decoder\n",
        "  reconstructed = decoder(z)\n",
        "\n",
        "  # define the inputs and outputs of the VAE\n",
        "  model = tf.keras.Model(inputs=inputs, outputs=reconstructed)\n",
        "\n",
        "  # add the KL loss\n",
        "  loss = kl_reconstruction_loss(inputs, z, mu, sigma)\n",
        "  model.add_loss(loss)\n",
        "  \n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_lbWSKbALf-"
      },
      "source": [
        "Helper function to return the encoder, decoder, and VAE models \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hnPo0Pr3nIts",
        "lines_to_next_cell": 2
      },
      "source": [
        "def get_models(input_shape, latent_dim):\n",
        "  \"\"\"Returns the encoder, decoder, and vae models\"\"\"\n",
        "\n",
        "  encoder, conv_shape = encoder_model(latent_dim=latent_dim, input_shape=input_shape)\n",
        "  decoder = decoder_model(latent_dim=latent_dim, conv_shape=conv_shape)\n",
        "  vae = vae_model(encoder, decoder, input_shape=input_shape)\n",
        "\n",
        "  return encoder, decoder, vae"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJsdzZTPVgOn"
      },
      "source": [
        "Summary of the models in the training loop.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHdr3CUznL5Z"
      },
      "source": [
        "encoder, decoder, vae = get_models(input_shape=(64,64,3,), latent_dim=LATENT_DIM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6IwN5vlAb5w"
      },
      "source": [
        "## Train the Model\n",
        "\n",
        "Losses, the optimizer, and the loss metric\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHPwSmZFnQ_2"
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.002)\n",
        "loss_metric = tf.keras.metrics.Mean()\n",
        "mse_loss = tf.keras.losses.MeanSquaredError()\n",
        "bce_loss = tf.keras.losses.BinaryCrossentropy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWRzFxYkAvXH"
      },
      "source": [
        "16 images in a 4x4 grid to show\n",
        "progress of image generation. \n",
        "Defining a utility function for that below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGe445j0nTmf"
      },
      "source": [
        "def generate_and_save_images(model, epoch, step, test_input):\n",
        "  \"\"\"Helper function to plot our 16 images\n",
        "\n",
        "  Args:\n",
        "\n",
        "  model -- the decoder model\n",
        "  epoch -- current epoch number during training\n",
        "  step -- current step number during training\n",
        "  test_input -- random tensor with shape (16, LATENT_DIM)\n",
        "  \"\"\"\n",
        "  predictions = model.predict(test_input)\n",
        "\n",
        "  fig = plt.figure(figsize=(4,4))\n",
        "\n",
        "  for i in range(predictions.shape[0]):\n",
        "      plt.subplot(4, 4, i+1)\n",
        "      img = predictions[i, :, :, :] * 255\n",
        "      img = img.astype('int32')\n",
        "      plt.imshow(img)\n",
        "      plt.axis('off')\n",
        "\n",
        "  # tight_layout minimizes the overlap between 2 sub-plots\n",
        "  fig.suptitle(\"epoch: {}, step: {}\".format(epoch, step))\n",
        "  plt.savefig('image_at_epoch_{:04d}_step{:04d}.png'.format(epoch, step))\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgJfazr6A_py"
      },
      "source": [
        "Workflow :\n",
        "\n",
        "* feed a training batch to the VAE model\n",
        "* compute the reconstruction loss (hint: use the **mse_loss** defined above instead of `bce_loss` in the ungraded lab, then multiply by the flattened dimensions of the image (i.e. 64 x 64 x 3)\n",
        "* add the KLD regularization loss to the total loss (you can access the `losses` property of the `vae` model)\n",
        "* get the gradients\n",
        "* use the optimizer to update the weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvL1bHXJnajM"
      },
      "source": [
        "# Training loop. Display generated images each epoch\n",
        "\n",
        "epochs = 20\n",
        "\n",
        "random_vector_for_generation = tf.random.normal(shape=[16, LATENT_DIM])\n",
        "generate_and_save_images(decoder, 0, 0, random_vector_for_generation)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  print('Start of epoch %d' % (epoch,))\n",
        "\n",
        "  # Iterate over the batches of the dataset.\n",
        "  for step, x_batch_train in enumerate(training_dataset):\n",
        "    with tf.GradientTape() as tape:\n",
        "      reconstructed = vae(x_batch_train)\n",
        "      # Compute reconstruction loss \n",
        "      flattened_inputs = tf.reshape(x_batch_train, shape=[-1])\n",
        "      flattened_outputs = tf.reshape(reconstructed, shape=[-1])\n",
        "      loss = mse_loss(flattened_inputs, flattened_outputs) * 64 * 64 * 3\n",
        "      loss += sum(vae.losses) \n",
        "\n",
        "      \n",
        "\n",
        "    grads = tape.gradient(loss, vae.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(grads, vae.trainable_weights))\n",
        "      \n",
        "    \n",
        "    loss_metric(loss)\n",
        "\n",
        "    if step % 10 == 0:\n",
        "      display.clear_output(wait=False)    \n",
        "      generate_and_save_images(decoder, epoch, step, random_vector_for_generation)\n",
        "    print('Epoch: %s step: %s mean loss = %s' % (epoch, step, loss_metric.result().numpy()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5wfzGfABny6"
      },
      "source": [
        "# Plot Reconstructed Images\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfIbqTIKSXEe"
      },
      "source": [
        "test_dataset = validation_dataset.take(1)\n",
        "output_samples = []\n",
        "\n",
        "for input_image in tfds.as_numpy(test_dataset):\n",
        "      output_samples = input_image\n",
        "\n",
        "idxs = np.random.choice(64, size=10)\n",
        "\n",
        "vae_predicted = vae.predict(test_dataset)\n",
        "display_results(output_samples[idxs], vae_predicted[idxs])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YKUOCA5BtAA"
      },
      "source": [
        "# Plot Generated Images\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylxL9z15ctsy"
      },
      "source": [
        "8x8 gallery of fake data generated from the model after 50 epochs\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1QwElgfg5TY6vCgI1FK6vdI8Bo6UZKfuX\" width=\"75%\" height=\"60%\"/>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCpTybvGSS6L"
      },
      "source": [
        "def plot_images(rows, cols, images, title):\n",
        "    '''Displays images in a grid.'''\n",
        "    grid = np.zeros(shape=(rows*64, cols*64, 3))\n",
        "    for row in range(rows):\n",
        "        for col in range(cols):\n",
        "            grid[row*64:(row+1)*64, col*64:(col+1)*64, :] = images[row*cols + col]\n",
        "\n",
        "    plt.figure(figsize=(12,12))       \n",
        "    plt.imshow(grid)\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "# initialize random inputs\n",
        "test_vector_for_generation = tf.random.normal(shape=[64, LATENT_DIM])\n",
        "\n",
        "# get predictions from the decoder model\n",
        "predictions= decoder.predict(test_vector_for_generation)\n",
        "\n",
        "# plot the predictions\n",
        "plot_images(8,8,predictions,'Generated Images')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4IixoasCfoR"
      },
      "source": [
        "### Saving the .h5 model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9E8qwDAVMPs"
      },
      "source": [
        "vae.save(\"anime.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtO0D48lGvED"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}